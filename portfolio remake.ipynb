{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:white; text-align:center; padding:20px;\">\n",
    "    <h2 style=\"color:black; font-family: Verdana, sans-serif;\"><strong>Giant Hogweed Detection</strong></h2>\n",
    "    <p style=\"font-size: 14px; color: black; font-family: Verdana, sans-serif;\"> \n",
    "        <table style=\"margin: auto; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <th style=\"border: 0;\">Names</th>\n",
    "                <th style=\"border: 0;\">GitHub Username</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Vince Ammerlaan</td>\n",
    "                <td style=\"border: 0;\">Vince16270</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Tommi Lander</td>\n",
    "                <td style=\"border: 0;\">tommi7</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Nima Ghafar</td>\n",
    "                <td style=\"border: 0;\">NimaGhafar</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; margin-top:20px;\">\n",
    "    <img src=\"https://lizzieharper.co.uk/wp-content/uploads/2023/03/Giant-Hogweed-Heracleum-mantegazzianum-lo-371x525.jpg\" alt=\"genres\" style=\"max-width: 100%; max-height: 100%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Client: Sieuwe Elferink\n",
    "- Instructors: Bob Tuinman, Pascal van den Bosch\n",
    "- Date: 21-6-2024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Table of contents </strong></h2>\n",
    "</div>\n",
    "<ul style=\"padding: 0; list-style: none;\">\n",
    "    <li style=\"text-align: center;\"><a href=\"#1.0\">Chapter 1: Project</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#1.0\">Chapter 1: Loading Data</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#2.0\">Chapter 2: Exploratory Data Analysis</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#3.0\">Chapter 3: Model</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#5.0\">Chapter 4: Model evaluatie</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#6.0\">Chapter 5: Eigen data uittesten</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#7.0\">Chapter 6: API voor MRR</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#8.0\">Chapter 7: Bronnenlijst</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 1: Project </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The giant hogweed is a plant that causes a lot of nuisance and if you touch it, you can get severe burns. This is due to the sap of the plant. The common hogweed, on the other hand, does not cause such problems and is much less dangerous. You can distinguish the giant hogweed from the common hogweed by its coarse, large leaves and rapid growth compared to the more modest, finer leaves of the common hogweed.\n",
    "\n",
    "Our project faces several complications. One of the biggest challenges is the lack of available drone data, which is essential for training the model to detect the hogweed. Since we do not have this data available, we have to make our own drone recordings and then create masks for image recognition. This process is very time-consuming.\n",
    "\n",
    "In addition, there are other challenges associated with obtaining this data. Flying drones requires specific permits and safety measures, especially in areas where the giant hogweed grows, which are often difficult to access.\n",
    "\n",
    "Despite these challenges, we are determined to find solutions and make our project a success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 2: Loading data </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import concatenate, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Own defined functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie om maskers voor te verwerken om binaire waarden te garanderen\n",
    "def preprocess_mask(mask):\n",
    "    mask = np.where(mask > 0, 1, 0).astype(np.uint8)\n",
    "    return mask\n",
    "\n",
    "# Functie om pixels met 1 of 0 te tellen\n",
    "def count_pixels(mask):\n",
    "    unique, counts = np.unique(mask, return_counts=True)\n",
    "    pixel_count = dict(zip(unique, counts))\n",
    "    return pixel_count\n",
    "\n",
    "# Functie om afbeeldingen weer te geven\n",
    "def display_images(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Drempel toepassen op voorspelde maskers voor binaire segmentatie\n",
    "def threshold_predictions(predictions, threshold=0.5):\n",
    "    return (predictions > threshold).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image, patch_size=256):\n",
    "    patches = []\n",
    "    h, w, _ = image.shape\n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            patch = image[i:i+patch_size, j:j+patch_size, :]\n",
    "            if patch.shape[0] != patch_size or patch.shape[1] != patch_size:\n",
    "                pad_h = patch_size - patch.shape[0]\n",
    "                pad_w = patch_size - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "def merge_patches(patches, image_shape, patch_size=256):\n",
    "    h, w, c = image_shape\n",
    "    reconstructed_image = np.zeros((h, w, 1))\n",
    "    patch_index = 0\n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            patch = patches[patch_index]\n",
    "            patch_h, patch_w, _ = patch.shape\n",
    "            if i + patch_h > h or j + patch_w > w:\n",
    "                patch = patch[:h-i, :w-j, :]  # Adjust the patch size to fit the image dimensions\n",
    "            reconstructed_image[i:i+patch_h, j:j+patch_w, :] = patch\n",
    "            patch_index += 1\n",
    "    return reconstructed_image\n",
    "\n",
    "# Laad je eigen afbeelding\n",
    "def load_single_image(image_path, target_size=None):\n",
    "    image = load_img(image_path, target_size=target_size)\n",
    "    image = img_to_array(image) / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    \"\"\"\n",
    "    Deze functie laat twee grafieken zien. De eerste toont de loss tijdens training en validatie over de epochs. \n",
    "    De tweede toont de IoU tijdens training en validatie over de epochs.\n",
    "\n",
    "    Parameters:\n",
    "        history: Dit is het object dat wordt teruggegeven door de `fit` methode van een Keras model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='#1f77b4', linestyle='-', linewidth=2, alpha=0.8)\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='#ff7f0e', linestyle='--', linewidth=2, alpha=0.8)\n",
    "    plt.title('Training and Validation Losses', fontdict={'fontsize': 18, 'fontweight': 'bold', 'color': '#333333'})\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.legend(fontsize=12, loc='upper right')\n",
    "    plt.grid(color='gray', linestyle='-', linewidth=1.5, alpha=0.5)\n",
    "    plt.fill_between(range(len(history.history['loss'])), history.history['loss'], history.history['val_loss'], color='lightgray', alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['IoU'], label='Training IoU', color='#1f77b4', linestyle='-', linewidth=2, alpha=0.8)\n",
    "    plt.plot(history.history['val_IoU'], label='Validation IoU', color='#ff7f0e', linestyle='--', linewidth=2, alpha=0.8)\n",
    "    plt.title('Training and Validation IoU', fontdict={'fontsize': 18, 'fontweight': 'bold', 'color': '#333333'})\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('IoU', fontsize=14)\n",
    "    plt.legend(fontsize=12, loc='lower right')\n",
    "    plt.grid(color='gray', linestyle='-', linewidth=1.5, alpha=0.5)\n",
    "    plt.fill_between(range(len(history.history['IoU'])), history.history['IoU'], history.history['val_IoU'], color='lightgray', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie om alleen de input en de voorspelde maskers weer te geven\n",
    "def display_prediction_images(input_image, predicted_mask):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(input_image))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(predicted_mask.squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hier knippen we de data op in allemaal kleine afbeeldingen van 256x256, zodat we meer data beschikbaar hebben en de foto's niet te groot zijn om inteladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\n",
    "def split_and_save_images(image_dir, output_dir, patch_size=256):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    file_names = os.listdir(image_dir)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        image_path = os.path.join(image_dir, file_name)\n",
    "        if os.path.isfile(image_path):\n",
    "            image = Image.open(image_path)\n",
    "            width, height = image.size\n",
    "\n",
    "            num_patches_x = width // patch_size\n",
    "            num_patches_y = height // patch_size\n",
    "\n",
    "            for i in range(num_patches_x):\n",
    "                for j in range(num_patches_y):\n",
    "                    left = i * patch_size\n",
    "                    upper = j * patch_size\n",
    "                    right = (i + 1) * patch_size\n",
    "                    lower = (j + 1) * patch_size\n",
    "                    \n",
    "                    patch = image.crop((left, upper, right, lower))\n",
    "                    patch_name = f\"{file_name.split('.')[0]}_patch_{i}_{j}.jpg\"\n",
    "                    patch.save(os.path.join(output_dir, patch_name))\n",
    "\n",
    "image_dir = 'eigen masks/images'\n",
    "output_dir = 'eigen masks/image_patches'\n",
    "split_and_save_images(image_dir, output_dir)\n",
    "\n",
    "mask_dir = 'eigen masks/masks'\n",
    "output_dir = 'eigen masks/mask_patches'\n",
    "split_and_save_images(mask_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading images & masks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'zelf gelabelde data/image_patches'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m mask_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzelf gelabelde data/mask_patches\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m---> 28\u001b[0m total_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     29\u001b[0m total_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(mask_folder))\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Bereken steps_per_epoch en validation_steps\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'zelf gelabelde data/image_patches'"
     ]
    }
   ],
   "source": [
    "def batch_generator(image_folder, mask_folder, batch_size):\n",
    "    image_files = os.listdir(image_folder)\n",
    "    mask_files = os.listdir(mask_folder)\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_masks = []\n",
    "        for i in range(batch_size):\n",
    "            img_file = image_files[i % len(image_files)]\n",
    "            mask_file = mask_files[i % len(mask_files)]\n",
    "            img_path = os.path.join(image_folder, img_file)\n",
    "            mask_path = os.path.join(mask_folder, mask_file)\n",
    "\n",
    "            image = load_img(img_path, target_size=(256, 256))\n",
    "            mask = load_img(mask_path, target_size=(256, 256), color_mode=\"grayscale\")\n",
    "\n",
    "            image = img_to_array(image) / 255.0\n",
    "            mask = img_to_array(mask)\n",
    "            mask = preprocess_mask(mask)\n",
    "\n",
    "            batch_images.append(image)\n",
    "            batch_masks.append(mask)\n",
    "\n",
    "        yield np.array(batch_images), np.array(batch_masks)\n",
    "\n",
    "image_folder = 'zelf gelabelde data/image_patches'\n",
    "mask_folder = 'zelf gelabelde data/mask_patches'\n",
    "batch_size = 64\n",
    "total_images = len(os.listdir(image_folder))\n",
    "total_masks = len(os.listdir(mask_folder))\n",
    "\n",
    "# Bereken steps_per_epoch en validation_steps\n",
    "steps_per_epoch = total_images // batch_size\n",
    "validation_steps = (total_images * 0.2) // batch_size\n",
    "\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "generator = batch_generator(image_folder, mask_folder, batch_size=batch_size)\n",
    "batch_images, batch_masks = next(generator)\n",
    "train_images, test_images, train_masks, test_masks = train_test_split(batch_images, batch_masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Voorbeeldweergave van een afbeelding en masker\n",
    "sample_image, sample_mask = train_images[17], train_masks[17]\n",
    "display_list = [sample_image, sample_mask]\n",
    "display_images(display_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 3: Exploratory Data Analysis </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_without_masks = sum(np.all(mask == 0) for mask in train_masks)\n",
    "percentage_without_masks = num_images_without_masks / len(train_masks) * 100\n",
    "\n",
    "print(f\"Aantal afbeeldingen in de dataset: {len(train_masks)}\")\n",
    "print(f\"Aantal afbeeldingen met maskers: {len(train_masks) - num_images_without_masks}\")\n",
    "print(f\"Aantal afbeeldingen zonder maskers: {num_images_without_masks}\")\n",
    "print(f\"Percentage afbeeldingen met maskers: {100 - round(percentage_without_masks, 2)}%\")\n",
    "print(f\"Percentage afbeeldingen zonder maskers: {round(percentage_without_masks, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfbeelding lengte en breedte: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mimages\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Afbeelding lengte en breedte: {images[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mimages\u001b[49m)):\n\u001b[0;32m      4\u001b[0m     height, width, channels \u001b[38;5;241m=\u001b[39m images[i]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      5\u001b[0m     size \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m*\u001b[39m width\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(images)):\n",
    "    height, width, channels = images[i].shape\n",
    "    size = height * width\n",
    "    has_mask = np.any(masks[i])\n",
    "    mask_coverage = np.sum(masks[i]) / size\n",
    "    aspect_ratio = width / height\n",
    "    avg_color = np.mean(images[i], axis=(0, 1)) / 255  \n",
    "    avg_color_hex = mcolors.rgb2hex(avg_color)\n",
    "    \n",
    "    color_std = np.std(images[i], axis=(0, 1)) / 255\n",
    "    brightness = np.mean(images[i]) / 255\n",
    "    gray_image = rgb2gray(images[i])\n",
    "    contrast = np.std(gray_image)\n",
    "\n",
    "    data.append([i, width, height, size, has_mask, mask_coverage, avg_color, avg_color_hex, color_std, brightness, contrast])\n",
    "\n",
    "images_info = pd.DataFrame(data, columns=['image_id', 'Breedte', 'Hoogte', 'Grootte'\n",
    "                                          , 'Heeft een mask', 'Mask bedekking', 'Verdeling kleuren'\n",
    "                                          , 'Gemiddelde kleur HTML', 'Kleurvariatie', 'Helderheid'\n",
    "                                          , 'Contrast'])\n",
    "\n",
    "images_info.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1.⁠ ⁠⁠ image_id ⁠: Dit is een unieke identificatie voor elke afbeelding in de dataset. In dit geval is het de index van de afbeelding in de lijst ⁠ images ⁠. Bijvoorbeeld, ⁠ 0 ⁠ betekent dat dit de eerste afbeelding in de lijst is. Het is belangrijk om elke afbeelding te kunnen identificeren, vooral als je later naar specifieke afbeeldingen wilt verwijzen.\n",
    "\n",
    " 2.⁠ ⁠⁠ Breedte ⁠: Dit is de breedte van de afbeelding in pixels. In dit geval is de breedte ⁠ 224 ⁠ pixels. Het kennen van de afmetingen van de afbeelding kan nuttig zijn bij het analyseren van de afbeelding, bijvoorbeeld om te begrijpen hoe de grootte van de afbeelding de maskerdekking beïnvloedt.\n",
    "\n",
    " 3.⁠ ⁠⁠ Hoogte ⁠: Dit is de hoogte van de afbeelding in pixels. In dit geval is de hoogte ⁠ 224 ⁠ pixels. Net als de breedte kan de hoogte nuttig zijn bij het analyseren van de afbeelding.\n",
    "\n",
    " 4.⁠ ⁠⁠ Grootte ⁠: Dit is het totale aantal pixels in de afbeelding, berekend als breedte x hoogte. In dit geval is de grootte ⁠ 50176 ⁠ pixels. Dit kan nuttig zijn bij het analyseren van de afbeelding, bijvoorbeeld om te begrijpen hoe de grootte van de afbeelding de maskerdekking beïnvloedt.\n",
    "\n",
    " 5.⁠ ⁠⁠ Heeft een mask ⁠: Dit geeft aan of de afbeelding een bijbehorend masker heeft. In dit geval heeft de afbeelding geen masker (⁠ False ⁠). Dit kan nuttig zijn bij het analyseren van de afbeelding, bijvoorbeeld om te begrijpen hoe de aanwezigheid van een masker de andere kenmerken van de afbeelding beïnvloedt.\n",
    "\n",
    " 6.⁠ ⁠⁠ Mask bedekking ⁠: Dit is het percentage van de afbeelding dat wordt bedekt door het masker, berekend als het aantal maskerpixels gedeeld door het totale aantal pixels. In dit geval is de maskerdekking ⁠ 0.000000 ⁠, wat betekent dat er geen masker is. Dit kan nuttig zijn bij het analyseren van de afbeelding, bijvoorbeeld om te begrijpen hoe de maskerdekking de andere kenmerken van de afbeelding beïnvloedt.\n",
    "\n",
    " 7.⁠ ⁠⁠ Verdeling kleuren ⁠: Dit is de gemiddelde kleur van de afbeelding, berekend als de gemiddelde RGB-waarde van alle pixels en genormaliseerd tussen 0 en 1. In dit geval is de gemiddelde kleur ⁠ [0.3177765, 0.31320137, 0.20988709] ⁠, wat een donkerbruine kleur is. Het kennen van de gemiddelde kleur kan nuttig zijn bij het analyseren van de afbeelding, zodat je weet hoe verschillend de kleuren zijn.\n",
    "\n",
    " 8.⁠ ⁠⁠ Gemiddelde kleur HTML ⁠: Dit is de gemiddelde kleur van de afbeelding, omgezet naar een HTML-kleur (hexadecimale kleurcode). In dit geval is de gemiddelde kleur ⁠ #515036 ⁠, wat overeenkomt met de RGB-kleur in de 'Verdeling kleuren' kolom. Het kennen van de HTML-kleur kan nuttig zijn voor visualisatiedoeleinden, bijvoorbeeld om een overzicht van de gemiddelde kleuren van de afbeeldingen te maken.\n",
    "\n",
    " 9.⁠ ⁠⁠ Kleurvariatie ⁠: Dit is de standaardafwijking van de kleuren in de afbeelding, berekend als de standaardafwijking van de RGB-waarden van alle pixels en genormaliseerd tussen 0 en 1. In dit geval is de kleurvariatie ⁠ [0.18607375, 0.18607375, 0.18607375] ⁠, wat betekent dat de kleuren in de afbeelding niet erg variëren. Het kennen van de kleurvariatie kan nuttig zijn bij het analyseren van de afbeelding, bijvoorbeeld om te begrijpen hoe uniform de kleuren zijn.\n",
    "\n",
    "10.⁠ ⁠⁠ Helderheid ⁠: Dit is de gemiddelde helderheid van de afbeelding, berekend als de gemiddelde helderheid van alle pixels en genormaliseerd tussen 0 en 1. In dit geval is de helderheid ⁠ 0.31320137 ⁠, wat betekent dat de afbeelding niet erg helder is. Het kennen van de helderheid kan nuttig zijn bij het analyseren van de afbeelding, bijvoorbeeld om te begrijpen hoe licht of donker de afbeelding is.\n",
    "\n",
    "11.⁠ ⁠⁠ Contrast ⁠: Dit is de contrastwaarde van de afbeelding, berekend als de standaardafwijking van de grijswaarden van de afbeelding. In dit geval is het contrast ⁠ 0.18607375 ⁠, wat betekent dat de afbeelding een gemiddeld contrast heeft. Het kennen van het contrast kan nuttig zijn bij het analyseren van de afbeelding, bijvoorbeeld om te begrijpen hoe scherp de overgangen tussen lichte en donkere delen zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_info[['Kleurvariatie', 'Helderheid', 'Contrast']].hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(images_info['Mask bedekking'], bins=50)\n",
    "plt.xlabel('Mask Coverage')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of mask coverage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dat er veel afbeeldingen zijn zonder of weinig maskers. Dit is dan moeilijk voor het model om te leren. \n",
    "Wel is dit nodig, omdat we kunnen zien dat het model ook werkt als er geen berenklauw te zien is. \n",
    "Zoals eerder is aangegeven is 17% van de afbeeldingen zonder masker. Dit is een prima percentage om mee te werken, omdat het model ook moet leren om afbeeldingen zonder masker te herkennen meestal afbeeldingen zijn zonder berenklauwen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range=20,\n",
    "                     width_shift_range=0.2,\n",
    "                     height_shift_range=0.2,\n",
    "                     shear_range=0.2,\n",
    "                     zoom_range=0.2,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "image_datagen.fit(train_images, augment=True, seed=seed)\n",
    "mask_datagen.fit(train_masks, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow(train_images, batch_size=batch_size, seed=seed)\n",
    "mask_generator = mask_datagen.flow(train_masks, batch_size=batch_size, seed=seed)\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 4: Used Models </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models used:**\n",
    " \n",
    "- Resnet\n",
    "- U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**U-net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IoU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 52\u001b[0m\n\u001b[0;32m     48\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[IoU])\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43munet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[18], line 48\u001b[0m, in \u001b[0;36munet_model\u001b[1;34m(input_size)\u001b[0m\n\u001b[0;32m     45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(c9)\n\u001b[0;32m     47\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[inputs], outputs\u001b[38;5;241m=\u001b[39m[outputs])\n\u001b[1;32m---> 48\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[43mIoU\u001b[49m])\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IoU' is not defined"
     ]
    }
   ],
   "source": [
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    \n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    \n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "    \n",
    "    # Decoder\n",
    "    u6 = UpSampling2D((2, 2))(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "    \n",
    "    u7 = UpSampling2D((2, 2))(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "    \n",
    "    u8 = UpSampling2D((2, 2))(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "    \n",
    "    u9 = UpSampling2D((2, 2))(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[IoU])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = unet_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 5: Using own data on trained models </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image, patch_size=256):\n",
    "    patches = []\n",
    "    h, w, _ = image.shape\n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            patch = image[i:i+patch_size, j:j+patch_size, :]\n",
    "            if patch.shape[0] != patch_size or patch.shape[1] != patch_size:\n",
    "                pad_h = patch_size - patch.shape[0]\n",
    "                pad_w = patch_size - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "def merge_patches(patches, image_shape, patch_size=256):\n",
    "    h, w, c = image_shape\n",
    "    reconstructed_image = np.zeros((h, w, 1))\n",
    "    patch_index = 0\n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            patch = patches[patch_index]\n",
    "            patch_h, patch_w, _ = patch.shape\n",
    "            reconstructed_image[i:i+patch_h, j:j+patch_w, :] = patch\n",
    "            patch_index += 1\n",
    "    return reconstructed_image\n",
    "\n",
    "# Laad je eigen afbeelding\n",
    "def load_single_image(image_path, target_size=(256, 256)):\n",
    "    image = load_img(image_path, target_size=target_size)\n",
    "    image = img_to_array(image) / 255.0\n",
    "    return image\n",
    "\n",
    "# Geef het pad van je eigen afbeelding\n",
    "own_image_path = 'eigen data/your_image.jpg'\n",
    "own_image = load_single_image(own_image_path, target_size=None)  # Load original size\n",
    "\n",
    "# Snijd de afbeelding in stukken van 256x256\n",
    "patches = split_image(own_image, patch_size=256)\n",
    "\n",
    "# Voer voorspellingen uit voor elk stuk\n",
    "patches_array = np.array(patches)\n",
    "pred_patches = model.predict(patches_array)\n",
    "pred_patches_thresholded = threshold_predictions(pred_patches, threshold=0.5)\n",
    "\n",
    "# Voeg de voorspelde stukken samen tot één volledige afbeelding\n",
    "reconstructed_mask = merge_patches(pred_patches_thresholded, own_image.shape, patch_size=256)\n",
    "\n",
    "# Toon de resultaten\n",
    "display_prediction_images(own_image, reconstructed_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=3, validation_data=(test_images, test_masks), validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 6: Best Model </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_masks = model.predict(test_images)\n",
    "pred_masks_thresholded = threshold_predictions(pred_masks, threshold=0.5)\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    display_images([test_images[i], test_masks[i], pred_masks_thresholded[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
